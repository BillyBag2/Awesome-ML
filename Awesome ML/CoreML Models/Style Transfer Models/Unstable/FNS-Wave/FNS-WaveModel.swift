//
//  FNS-WaveModel.swift
//  Awesome ML
//
//  Created by Eugene Bokhan on 4/8/18.
//  Copyright Â© 2018 Eugene Bokhan. All rights reserved.
//

import Foundation

public let fnsWaveModel = CoreMLModel(name: "FNS-Wave", coreMLType: .fnsWave, shortDescription: "Feedforward style transfer", detailedDescription: "# Authors and Contributors \n Author: Justin Johnson. \n # Abstract \n We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results. \n ## Input  \n A color (720x720) image. \n ## Output \n Stylized image. \n ## License \n Public Domain", image: #imageLiteral(resourceName: "FNS-Wave Cover"), inputWidth: 720, inputHeight: 720, remoteURL: URL(string: "https://github.com/eugenebokhan/Awesome-ML/raw/master/CoreML%20Models/Style%20Transfer/wave.mlmodel")!, remoteZipURL: nil, license: "MIT")


